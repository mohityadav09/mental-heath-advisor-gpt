{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers -q\n!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:21:38.658904Z","iopub.execute_input":"2024-06-26T05:21:38.659183Z","iopub.status.idle":"2024-06-26T05:22:29.721113Z","shell.execute_reply.started":"2024-06-26T05:21:38.659155Z","shell.execute_reply":"2024-06-26T05:22:29.720185Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: auto-gptq in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.30.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.19.2)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\nRequirement already satisfied: gekko in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.1.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.41.2)\nRequirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.3.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.19.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.4)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.20.0)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12.1)\nRequirement already satisfied: transformers<4.42.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (4.41.2)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.23.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.19.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.3.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.42.0,>=4.26.0->transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.4.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.42.0,>=4.26.0->optimum) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import prepare_model_for_kbit_training\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import load_dataset\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:22:29.723170Z","iopub.execute_input":"2024-06-26T05:22:29.723469Z","iopub.status.idle":"2024-06-26T05:23:00.128347Z","shell.execute_reply.started":"2024-06-26T05:22:29.723439Z","shell.execute_reply":"2024-06-26T05:23:00.127339Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-06-26 05:22:41.070161: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-26 05:22:41.070276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-26 05:22:41.330849: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n                                             trust_remote_code=False, # prevents running custom model files on your machine\n                                             revision=\"main\") # which version of model to use in repo","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:23:57.013607Z","iopub.execute_input":"2024-06-26T05:23:57.013928Z","iopub.status.idle":"2024-06-26T05:23:58.005396Z","shell.execute_reply.started":"2024-06-26T05:23:57.013901Z","shell.execute_reply":"2024-06-26T05:23:58.004545Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c83c304409541f398e4a18e7cc55c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d95d20c5634ef6ab53781887cb0d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1937152870ea4ba88721a87b7eb13644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127351337a754a0abae3ff5323d27548"}},"metadata":{}}]},{"cell_type":"code","source":"model.train() # model in training mode (dropout modules are activated)\n\n# enable gradient check pointing\nmodel.gradient_checkpointing_enable()\n\n# enable quantized training\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:25:12.257884Z","iopub.execute_input":"2024-06-26T05:25:12.258707Z","iopub.status.idle":"2024-06-26T05:25:12.272402Z","shell.execute_reply.started":"2024-06-26T05:25:12.258678Z","shell.execute_reply":"2024-06-26T05:25:12.271617Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# LoRA trainable version of model\nmodel = get_peft_model(model, config)\n\n# trainable parameter count\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:25:32.781675Z","iopub.execute_input":"2024-06-26T05:25:32.782530Z","iopub.status.idle":"2024-06-26T05:25:32.890212Z","shell.execute_reply.started":"2024-06-26T05:25:32.782498Z","shell.execute_reply":"2024-06-26T05:25:32.889241Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n","output_type":"stream"}]},{"cell_type":"code","source":"data = load_dataset(\"Amod/mental_health_counseling_conversations\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:28:06.164846Z","iopub.execute_input":"2024-06-26T05:28:06.165241Z","iopub.status.idle":"2024-06-26T05:28:08.555944Z","shell.execute_reply.started":"2024-06-26T05:28:06.165214Z","shell.execute_reply":"2024-06-26T05:28:08.555240Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5fac260e5049f7b5ef2929ad82f525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.79M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c431bdadfbe41b69037c0a05ad6442c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458a07d9748e4a00a2a891274f5ef490"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:31:12.597511Z","iopub.execute_input":"2024-06-26T05:31:12.597902Z","iopub.status.idle":"2024-06-26T05:31:12.603978Z","shell.execute_reply.started":"2024-06-26T05:31:12.597872Z","shell.execute_reply":"2024-06-26T05:31:12.603045Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Context', 'Response'],\n        num_rows: 3512\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"df = load_dataset(\"shawhin/shawgpt-youtube-comments\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:31:53.238279Z","iopub.execute_input":"2024-06-26T05:31:53.239015Z","iopub.status.idle":"2024-06-26T05:31:56.106773Z","shell.execute_reply.started":"2024-06-26T05:31:53.238979Z","shell.execute_reply":"2024-06-26T05:31:56.105795Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f867588a39642e49064aac1809f280d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/18.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"406fef48519d42d2aee673bdc2785d01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9242287f7f4845c1a936e73280bf5243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b602893dbf4f14aad22117c0065b05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af95bc8231cb46f384c08687effefabe"}},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:31:59.502628Z","iopub.execute_input":"2024-06-26T05:31:59.503457Z","iopub.status.idle":"2024-06-26T05:31:59.509159Z","shell.execute_reply.started":"2024-06-26T05:31:59.503423Z","shell.execute_reply":"2024-06-26T05:31:59.508193Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['example'],\n        num_rows: 50\n    })\n    test: Dataset({\n        features: ['example'],\n        num_rows: 9\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"model.eval() # model in evaluation mode (dropout modules are deactivated)\n\n# craft prompt\ncomment = \"i think i am not fit in this world\"\nprompt=f'''<s>[INST] {comment} [/INST]'''\n\n# tokenize input\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:51:35.904576Z","iopub.execute_input":"2024-06-26T05:51:35.905442Z","iopub.status.idle":"2024-06-26T05:54:06.142186Z","shell.execute_reply.started":"2024-06-26T05:51:35.905410Z","shell.execute_reply":"2024-06-26T05:54:06.141245Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s><s> [INST] i think i am not fit in this world [/INST] I'm here to listen and support you. It's important to remember that everyone has moments of feeling overwhelmed or uncertain about their place in the world. It's natural to have ups and downs in life. If you're feeling this way, it might be helpful to talk to someone about how you're feeling. This could be a trusted friend or family member, or a mental health professional. They can help you explore the reasons behind your feelings and provide you with strategies to cope. Remember, it's okay to ask for help when you need it. You are important and valued, and there are people who care about you. If you're having thoughts of\n","output_type":"stream"}]},{"cell_type":"code","source":"intstructions_string=f\"\"\"You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n\"\"\"\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n\nprompt = prompt_template(comment)\nprint(prompt)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:57:37.440232Z","iopub.execute_input":"2024-06-26T05:57:37.440972Z","iopub.status.idle":"2024-06-26T05:57:37.448017Z","shell.execute_reply.started":"2024-06-26T05:57:37.440939Z","shell.execute_reply":"2024-06-26T05:57:37.447010Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[INST] You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n \ni think i am not fit in this world \n[/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T05:58:18.320488Z","iopub.execute_input":"2024-06-26T05:58:18.320878Z","iopub.status.idle":"2024-06-26T06:00:48.886968Z","shell.execute_reply.started":"2024-06-26T05:58:18.320841Z","shell.execute_reply":"2024-06-26T06:00:48.885898Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n \ni think i am not fit in this world \n[/INST] I'm really sorry to hear that you're feeling this way, but I want you to know that it's important to remember that every person has value and worth, regardless of how they may feel about themselves in the moment. It's natural to have ups and downs in life, and it's okay to not feel like you fit in perfectly all the time.\n\nOne thing that might help is to focus on the things that bring you joy and fulfillment. It could be a hobby, a relationship, or even just a simple activity that makes you feel good. By focusing on these things, you may find that you feel more connected to the world around\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = data[\"train\"]\n\n# Convert the 'train' dataset to a list of dictionaries\ntrain_list = train_dataset.to_dict()\n\ncontext_list=train_list['Context']\nresponse_list=train_list['Response']\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:50.891267Z","iopub.execute_input":"2024-06-26T07:18:50.891654Z","iopub.status.idle":"2024-06-26T07:18:50.917976Z","shell.execute_reply.started":"2024-06-26T07:18:50.891622Z","shell.execute_reply":"2024-06-26T07:18:50.916976Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"instructions_string=f\"\"\"You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n\"\"\"\ntraining_examples=[]\ntest_examples=[]\nfor context,response in zip(context_list,response_list):\n    example=f\"<s>[INST] {instructions_string} \\n{context} \\n[/INST] \\n{response} \\n</s>\"\n    if len(training_examples) <=200:\n        training_examples.append(example)\n    elif len(test_examples) <=25:\n        test_examples.append(example) \n    else:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:51.907651Z","iopub.execute_input":"2024-06-26T07:18:51.908018Z","iopub.status.idle":"2024-06-26T07:18:51.918717Z","shell.execute_reply.started":"2024-06-26T07:18:51.907988Z","shell.execute_reply":"2024-06-26T07:18:51.917740Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"print(training_examples[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_examples[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.pad_token=tokenizer.eos_token\ntokenizer.truncation_side='left'\ntraining_encodings=tokenizer(training_examples,return_tensors='np',truncation=True,padding='max_length',max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:54.689359Z","iopub.execute_input":"2024-06-26T07:18:54.689742Z","iopub.status.idle":"2024-06-26T07:18:54.759229Z","shell.execute_reply.started":"2024-06-26T07:18:54.689710Z","shell.execute_reply":"2024-06-26T07:18:54.758195Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"test_encodings=tokenizer(training_examples,return_tensors='np',truncation=True,padding='max_length',max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:55.616159Z","iopub.execute_input":"2024-06-26T07:18:55.616516Z","iopub.status.idle":"2024-06-26T07:18:55.685406Z","shell.execute_reply.started":"2024-06-26T07:18:55.616486Z","shell.execute_reply":"2024-06-26T07:18:55.684267Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset,DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:56.511129Z","iopub.execute_input":"2024-06-26T07:18:56.511520Z","iopub.status.idle":"2024-06-26T07:18:56.519364Z","shell.execute_reply.started":"2024-06-26T07:18:56.511491Z","shell.execute_reply":"2024-06-26T07:18:56.517850Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"class textDataset(Dataset):\n    def __init__(self,encodings):\n        self.encodings=encodings\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self,idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item      ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:18:57.996650Z","iopub.execute_input":"2024-06-26T07:18:57.997092Z","iopub.status.idle":"2024-06-26T07:18:58.004707Z","shell.execute_reply.started":"2024-06-26T07:18:57.997056Z","shell.execute_reply":"2024-06-26T07:18:58.003576Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"train_data=textDataset(training_encodings)\ntrain_data[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=textDataset(test_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:00.089372Z","iopub.execute_input":"2024-06-26T07:19:00.089740Z","iopub.status.idle":"2024-06-26T07:19:00.095580Z","shell.execute_reply.started":"2024-06-26T07:19:00.089710Z","shell.execute_reply":"2024-06-26T07:19:00.094399Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:01.720775Z","iopub.execute_input":"2024-06-26T07:19:01.721711Z","iopub.status.idle":"2024-06-26T07:19:01.730912Z","shell.execute_reply.started":"2024-06-26T07:19:01.721666Z","shell.execute_reply":"2024-06-26T07:19:01.729792Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"model.train() # model in training mode (dropout modules are activated)\n\n# enable gradient check pointing\nmodel.gradient_checkpointing_enable()\n\n# enable quantized training\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:03.096952Z","iopub.execute_input":"2024-06-26T07:19:03.097934Z","iopub.status.idle":"2024-06-26T07:19:03.120381Z","shell.execute_reply.started":"2024-06-26T07:19:03.097899Z","shell.execute_reply":"2024-06-26T07:19:03.119250Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# LoRA config\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# LoRA trainable version of model\nmodel = get_peft_model(model, config)\n\n# trainable parameter count\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:04.115458Z","iopub.execute_input":"2024-06-26T07:19:04.115780Z","iopub.status.idle":"2024-06-26T07:19:04.215544Z","shell.execute_reply.started":"2024-06-26T07:19:04.115755Z","shell.execute_reply":"2024-06-26T07:19:04.214494Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n","output_type":"stream"}]},{"cell_type":"code","source":"lr = 2e-4\nbatch_size = 4\nnum_epochs = 10\n\n# define training arguments\ntraining_args = transformers.TrainingArguments(\n    output_dir= \"/kaggle/working/\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    logging_strategy=\"epoch\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=4,\n    warmup_steps=2,\n    fp16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=None\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:05.062265Z","iopub.execute_input":"2024-06-26T07:19:05.063051Z","iopub.status.idle":"2024-06-26T07:19:05.100153Z","shell.execute_reply.started":"2024-06-26T07:19:05.063021Z","shell.execute_reply":"2024-06-26T07:19:05.099159Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    args=training_args,\n    data_collator=data_collator\n)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:06.208626Z","iopub.execute_input":"2024-06-26T07:19:06.209001Z","iopub.status.idle":"2024-06-26T07:19:06.223323Z","shell.execute_reply.started":"2024-06-26T07:19:06.208971Z","shell.execute_reply":"2024-06-26T07:19:06.222247Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n\n# renable warnings\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-06-26T07:19:07.299097Z","iopub.execute_input":"2024-06-26T07:19:07.299425Z","iopub.status.idle":"2024-06-26T08:35:48.390186Z","shell.execute_reply.started":"2024-06-26T07:19:07.299400Z","shell.execute_reply":"2024-06-26T08:35:48.388983Z"},"trusted":true},"execution_count":117,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [120/120 1:16:12, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>3.113900</td>\n      <td>2.308430</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.919100</td>\n      <td>1.705455</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.608300</td>\n      <td>1.574059</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.530900</td>\n      <td>1.403755</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.362700</td>\n      <td>1.357315</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.311100</td>\n      <td>1.328080</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.386600</td>\n      <td>1.299816</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.180000</td>\n      <td>1.298420</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:36:11.676162Z","iopub.execute_input":"2024-06-26T08:36:11.676528Z","iopub.status.idle":"2024-06-26T08:38:19.578911Z","shell.execute_reply.started":"2024-06-26T08:36:11.676498Z","shell.execute_reply":"2024-06-26T08:38:19.577314Z"},"trusted":true},"execution_count":118,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [51/51 02:05]\n    </div>\n    "},"metadata":{}},{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 1.2984201908111572,\n 'eval_runtime': 127.8842,\n 'eval_samples_per_second': 1.572,\n 'eval_steps_per_second': 0.399,\n 'epoch': 9.411764705882353}"},"metadata":{}}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:21:21.818694Z","iopub.execute_input":"2024-06-26T09:21:21.819100Z","iopub.status.idle":"2024-06-26T09:21:21.845624Z","shell.execute_reply.started":"2024-06-26T09:21:21.819071Z","shell.execute_reply":"2024-06-26T09:21:21.844605Z"},"trusted":true},"execution_count":130,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"753f8138283a4a5b9ffbf79b4961f1e5"}},"metadata":{}}]},{"cell_type":"code","source":"hf_name = 'mohityadav' # your hf username or org name\nmodel_id = hf_name + \"/\" + \"mental-health-advisor-gpt\"","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:21:36.630992Z","iopub.execute_input":"2024-06-26T09:21:36.631896Z","iopub.status.idle":"2024-06-26T09:21:36.637843Z","shell.execute_reply.started":"2024-06-26T09:21:36.631861Z","shell.execute_reply":"2024-06-26T09:21:36.636604Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(model_id)\ntrainer.push_to_hub(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T09:21:37.141098Z","iopub.execute_input":"2024-06-26T09:21:37.141992Z","iopub.status.idle":"2024-06-26T09:21:40.703416Z","shell.execute_reply.started":"2024-06-26T09:21:37.141958Z","shell.execute_reply":"2024-06-26T09:21:40.702463Z"},"trusted":true},"execution_count":132,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/8.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11167390f4245319bbe54725259ddc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/8.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bf33b53fecb4949bfe1ee270b91f3e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 8 LFS files:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73725ffc834d4ad3a20f006044265d65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719385345.adc95cf38b20.34.0:   0%|          | 0.00/5.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcaf78a23a5e47b483dcea02b8e6e11b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719385423.adc95cf38b20.34.1:   0%|          | 0.00/5.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e9eb03f4bc42d4be1a888ab1f7f353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719386177.adc95cf38b20.34.3:   0%|          | 0.00/5.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"402f51fe43ea489190b58867b6205697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719385648.adc95cf38b20.34.2:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db800a29cc8d4c128f64fc38ea1cc592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719386347.adc95cf38b20.34.4:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"270ad8189a1f47c68bc1fb3bf8aeb901"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4988248ac7c84f19874d760049fb1697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1719391099.adc95cf38b20.34.5:   0%|          | 0.00/354 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fef5b0037414fec8f9b17e7f6067a84"}},"metadata":{}},{"execution_count":132,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/mohityadav/working/commit/a1847921592fd44bf07dbb87771cc87dff72b97c', commit_message='mohityadav/mental-health-advisor-gpt', commit_description='', oid='a1847921592fd44bf07dbb87771cc87dff72b97c', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"intstructions_string=f\"\"\"You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n\"\"\"\ncomment=\"I am not feeling good, i think life is boring\"\nprompt=f'''<s>[INST] {comment} [/INST]'''\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n\nprompt = prompt_template(comment)\nprint(prompt)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:54:32.053930Z","iopub.execute_input":"2024-06-26T08:54:32.054321Z","iopub.status.idle":"2024-06-26T08:54:32.063610Z","shell.execute_reply.started":"2024-06-26T08:54:32.054290Z","shell.execute_reply":"2024-06-26T08:54:32.062475Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"[INST] You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n \nI am not feeling good, i think life is boring \n[/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=200)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T08:54:32.752336Z","iopub.execute_input":"2024-06-26T08:54:32.752647Z","iopub.status.idle":"2024-06-26T08:58:07.759343Z","shell.execute_reply.started":"2024-06-26T08:54:32.752622Z","shell.execute_reply":"2024-06-26T08:58:07.758456Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] You are a compassionate and knowledgeable mental health advisor. Someone is sharing their mental \nstate or expressing doubts about their well-being. Provide the best advice and suggestions to help them navigate their\nfeelings. Offer empathetic, practical, and actionable guidance, and recommend helpful resources or techniques.\nEnsure your response is simple, relatable, encourages openness, and provides reassurance.\nplease provide the valuable advice for this comment.\n \nI am not feeling good, i think life is boring \n[/INST] \nLife is not supposed to be boring. It is a gift.  It is a journey.  It is a chance to learn and grow.  It is a chance to make a difference.  It is a chance to be happy.  It is a chance to be loved.  It is a chance to be successful.  It is a chance to be creative.  It is a chance to be a good person.  It is a chance to be a hero.  It is a chance to be a hero to someone.  It is a chance to be a hero to yourself.  It is a chance to be a hero to the world.  It is a chance to be a hero to the universe.  It is a chance to be a hero to God.  It is a chance to be a hero to God's people.  It is a chance to be a hero\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}